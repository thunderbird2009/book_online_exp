{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dfeb0cc",
   "metadata": {},
   "source": [
    "# Chapter 17: Bayesian Optimization Code Verification\n",
    "\n",
    "This notebook tests and verifies the Bayesian Optimization example from Chapter 17.\n",
    "\n",
    "We'll implement:\n",
    "1. Installation of required packages (Ax)\n",
    "2. The complete code example with working helper functions\n",
    "3. Validation that the workflow matches the diagram in Section 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f4034",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "\n",
    "First, let's install Ax and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f79def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ax (includes BoTorch as dependency)\n",
    "!pip install ax-platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951de652",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a49b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Use simplified Ax imports that work with current version\n",
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8935d9",
   "metadata": {},
   "source": [
    "## Step 3: Define Helper Functions\n",
    "\n",
    "These simulate the model training and A/B testing process. In production, these would integrate with your ML pipeline and experimentation platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_config(params):\n",
    "    \"\"\"\n",
    "    Simulate training a model with given hyperparameters.\n",
    "    \n",
    "    In production, this would:\n",
    "    - Load training data\n",
    "    - Initialize model with hyperparameters\n",
    "    - Train the model\n",
    "    - Return the trained model object\n",
    "    \n",
    "    For simulation, we just return a dict representing the model.\n",
    "    \"\"\"\n",
    "    model_id = f\"model_{hash(str(params)) % 10000}\"\n",
    "    print(f\"  [Training] {model_id} with params: {params}\")\n",
    "    return {\"id\": model_id, \"params\": params}\n",
    "\n",
    "\n",
    "def simulate_true_objective(params):\n",
    "    \"\"\"\n",
    "    Simulate the true (unknown) objective function.\n",
    "    \n",
    "    In reality, this is what we're trying to optimize through A/B tests.\n",
    "    Here we use a synthetic function with a known optimum for validation.\n",
    "    \n",
    "    True optimum is approximately:\n",
    "    - learning_rate: 0.01-0.03\n",
    "    - max_depth: 5-6\n",
    "    - min_samples_split: 5-8\n",
    "    - optimizer: adam\n",
    "    \"\"\"\n",
    "    lr = params['learning_rate']\n",
    "    depth = params['max_depth']\n",
    "    samples = params['min_samples_split']\n",
    "    opt = params['optimizer']\n",
    "    \n",
    "    # Synthetic objective function (CTR)\n",
    "    # Optimal around lr=0.02, depth=5, samples=6, optimizer=adam\n",
    "    lr_score = -100 * (np.log10(lr) + 1.7)**2  # Peak around 0.02\n",
    "    depth_score = -2 * (depth - 5.5)**2  # Peak around 5-6\n",
    "    samples_score = -0.5 * (samples - 6)**2  # Peak around 6\n",
    "    \n",
    "    # Optimizer bonus\n",
    "    opt_bonus = {'adam': 0.5, 'sgd': 0.0, 'rmsprop': 0.3}[opt]\n",
    "    \n",
    "    # Combine scores (base CTR around 4%)\n",
    "    base_ctr = 0.04\n",
    "    score = base_ctr + (lr_score + depth_score + samples_score) / 1000 + opt_bonus / 100\n",
    "    \n",
    "    return max(0.01, score)  # Ensure positive CTR\n",
    "\n",
    "\n",
    "def run_abn_test(trained_models, noise_level=0.0005):\n",
    "    \"\"\"\n",
    "    Simulate running an A/B/n test with multiple model variants.\n",
    "    \n",
    "    In production, this would:\n",
    "    - Deploy models to experimentation platform\n",
    "    - Allocate traffic to each variant\n",
    "    - Wait for statistical significance\n",
    "    - Collect metrics (mean + standard error)\n",
    "    \n",
    "    For simulation, we:\n",
    "    - Use the synthetic objective function\n",
    "    - Add noise to simulate variance\n",
    "    - Calculate standard error based on typical sample sizes\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for trial_index, model in trained_models:\n",
    "        params = model['params']\n",
    "        \n",
    "        # Simulate true CTR with noise\n",
    "        true_ctr = simulate_true_objective(params)\n",
    "        observed_ctr = true_ctr + np.random.normal(0, noise_level)\n",
    "        \n",
    "        # Simulate standard error (typical for A/B test with ~10k users per arm)\n",
    "        sem = noise_level + np.random.uniform(0, 0.0002)\n",
    "        \n",
    "        results[trial_index] = {\n",
    "            'ctr_mean': observed_ctr,\n",
    "            'ctr_sem': sem\n",
    "        }\n",
    "        \n",
    "        print(f\"  [A/B Test] Trial {trial_index}: CTR = {observed_ctr:.4f} ± {sem:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def check_convergence(client, threshold=0.0001):\n",
    "    \"\"\"\n",
    "    Check if Bayesian Optimization has converged.\n",
    "    \n",
    "    In production, you might check:\n",
    "    - Expected Improvement falls below threshold\n",
    "    - No improvement in best value for N iterations\n",
    "    - Target metric value achieved\n",
    "    \n",
    "    For simplicity, we return False to always run the full budget.\n",
    "    \"\"\"\n",
    "    # Could implement: check if max EI across search space < threshold\n",
    "    return False\n",
    "\n",
    "\n",
    "def deploy_to_production(model):\n",
    "    \"\"\"\n",
    "    Deploy the final optimized model to production.\n",
    "    \n",
    "    In production, this would:\n",
    "    - Save model artifacts\n",
    "    - Update model serving infrastructure\n",
    "    - Monitor deployment health\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Deployment] Model {model['id']} deployed to production!\")\n",
    "    print(f\"[Deployment] Final hyperparameters: {model['params']}\")\n",
    "\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf133ec",
   "metadata": {},
   "source": [
    "## Step 4: Configure Bayesian Optimization\n",
    "\n",
    "This matches the code example from Chapter 17, Section 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a70fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize Ax client with automatic generation strategy\n",
    "# Ax will automatically use SOBOL for initialization then switch to GPEI for BO\n",
    "ax_client = AxClient(verbose_logging=False)\n",
    "\n",
    "ax_client.create_experiment(\n",
    "    name=\"model_hyperparameter_tuning\",\n",
    "    parameters=[\n",
    "        {\"name\": \"learning_rate\", \"type\": \"range\", \"bounds\": [1e-4, 1e-1], \n",
    "         \"value_type\": \"float\", \"log_scale\": True},\n",
    "        {\"name\": \"max_depth\", \"type\": \"range\", \"bounds\": [3, 10], \n",
    "         \"value_type\": \"int\"},\n",
    "        {\"name\": \"min_samples_split\", \"type\": \"range\", \"bounds\": [2, 20], \n",
    "         \"value_type\": \"int\"},\n",
    "        {\"name\": \"optimizer\", \"type\": \"choice\", \"values\": [\"adam\", \"sgd\", \"rmsprop\"]},\n",
    "    ],\n",
    "    objectives={\"ctr\": ObjectiveProperties(minimize=False)},  # Maximize CTR\n",
    "    choose_generation_strategy_kwargs={\n",
    "        \"num_initialization_trials\": 5,  # SOBOL initialization trials\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Ax client configured successfully!\")\n",
    "print(f\"Generation Strategy: SOBOL (5 trials) → GP-based BO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f057f9d7",
   "metadata": {},
   "source": [
    "## Step 5: Run Bayesian Optimization Loop\n",
    "\n",
    "This executes the workflow from the diagram in Section 4.5:\n",
    "1. InitBatch / OptimizeBatch\n",
    "2. Batch Train Models\n",
    "3. Create A/B/n Test & Deploy\n",
    "4. Monitor & Collect Results\n",
    "5. Update GP\n",
    "6. Check Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb22050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Optimization loop (matches diagram flow)\n",
    "total_budget = 20  # Maximum number of trials\n",
    "batch_size = 3  # Number of configurations to evaluate in parallel\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "trials_completed = 0\n",
    "iteration = 0\n",
    "\n",
    "while trials_completed < total_budget:\n",
    "    iteration += 1\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ITERATION {iteration}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Generate batch of configurations (InitBatch or OptimizeBatch from diagram)\n",
    "    print(\"\\n[Step 1] Generate Batch of Configurations\")\n",
    "    trial_configs = []\n",
    "    current_batch_size = min(batch_size, total_budget - trials_completed)\n",
    "    \n",
    "    for i in range(current_batch_size):\n",
    "        parameters, trial_index = ax_client.get_next_trial()\n",
    "        trial_configs.append((trial_index, parameters))\n",
    "        print(f\"  Trial {trial_index}: {parameters}\")\n",
    "    \n",
    "    # Batch Train Models: Train all models with selected hyperparameters\n",
    "    print(\"\\n[Step 2] Batch Train Models\")\n",
    "    trained_models = []\n",
    "    for trial_index, params in trial_configs:\n",
    "        model = train_model_with_config(params)\n",
    "        trained_models.append((trial_index, model))\n",
    "    \n",
    "    # Create A/B/n Test & Deploy: Deploy all models to experimentation platform\n",
    "    print(\"\\n[Step 3] Create A/B/n Test & Deploy All Models\")\n",
    "    experiment_results = run_abn_test(trained_models)\n",
    "    \n",
    "    # Monitor & Collect Results: Wait for statistical significance\n",
    "    print(\"\\n[Step 4] Update GP with Results\")\n",
    "    for trial_index, model in trained_models:\n",
    "        ctr_mean = experiment_results[trial_index][\"ctr_mean\"]\n",
    "        ctr_sem = experiment_results[trial_index][\"ctr_sem\"]\n",
    "        \n",
    "        # Update GP with observation\n",
    "        ax_client.complete_trial(\n",
    "            trial_index=trial_index,\n",
    "            raw_data={\"ctr\": (ctr_mean, ctr_sem)}  # Mean and SEM\n",
    "        )\n",
    "        print(f\"  Trial {trial_index} completed and added to GP\")\n",
    "        trials_completed += 1\n",
    "    \n",
    "    # CheckBudget: Evaluate convergence\n",
    "    if check_convergence(ax_client):\n",
    "        print(f\"\\nConverged after {trials_completed} trials\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZATION COMPLETE\")\n",
    "print(f\"Total trials: {trials_completed}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609eda96",
   "metadata": {},
   "source": [
    "## Step 6: Select Best Configuration and Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Select Best Configuration\n",
    "print(\"\\n[Step 5] Select Best Configuration\")\n",
    "best_parameters, metrics = ax_client.get_best_parameters()\n",
    "\n",
    "print(f\"\\nBest configuration found:\")\n",
    "for param, value in best_parameters.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nExpected CTR: {metrics[0]['ctr']:.4f} ± {metrics[1]['ctr']['ctr']:.4f}\")\n",
    "\n",
    "# Step 7: Train Final Production Model\n",
    "print(\"\\n[Step 6] Train Final Production Model\")\n",
    "final_model = train_model_with_config(best_parameters)\n",
    "deploy_to_production(final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b472e",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Optimization Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee52fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get optimization trace\n",
    "trials_df = ax_client.get_trials_data_frame()\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Bayesian Optimization Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: CTR over trials\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(trials_df.index, trials_df['ctr'], 'o-', alpha=0.6, label='Observed CTR')\n",
    "ax1.axhline(y=trials_df['ctr'].max(), color='r', linestyle='--', label='Best CTR', alpha=0.5)\n",
    "ax1.set_xlabel('Trial Number')\n",
    "ax1.set_ylabel('CTR')\n",
    "ax1.set_title('CTR Performance Over Trials')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Best CTR so far (cumulative max)\n",
    "ax2 = axes[0, 1]\n",
    "best_so_far = trials_df['ctr'].cummax()\n",
    "ax2.plot(best_so_far.index, best_so_far, 'g-', linewidth=2)\n",
    "ax2.fill_between(best_so_far.index, best_so_far, alpha=0.3)\n",
    "ax2.set_xlabel('Trial Number')\n",
    "ax2.set_ylabel('Best CTR So Far')\n",
    "ax2.set_title('Optimization Progress (Cumulative Best)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Learning rate exploration\n",
    "ax3 = axes[1, 0]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(trials_df)))\n",
    "scatter = ax3.scatter(trials_df['learning_rate'], trials_df['ctr'], \n",
    "                     c=trials_df.index, cmap='viridis', s=100, alpha=0.6, edgecolors='black')\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_xlabel('Learning Rate (log scale)')\n",
    "ax3.set_ylabel('CTR')\n",
    "ax3.set_title('Learning Rate vs CTR')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax3, label='Trial Number')\n",
    "\n",
    "# Plot 4: Max depth exploration\n",
    "ax4 = axes[1, 1]\n",
    "scatter2 = ax4.scatter(trials_df['max_depth'], trials_df['ctr'], \n",
    "                      c=trials_df.index, cmap='viridis', s=100, alpha=0.6, edgecolors='black')\n",
    "ax4.set_xlabel('Max Depth')\n",
    "ax4.set_ylabel('CTR')\n",
    "ax4.set_title('Max Depth vs CTR')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=ax4, label='Trial Number')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('../Images/ch17_bayesian_optimization_results.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Figure saved to: ../Images/ch17_bayesian_optimization_results.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429daaf",
   "metadata": {},
   "source": [
    "## Step 8: Analysis and Validation\n",
    "\n",
    "Let's verify that Bayesian Optimization performed better than random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with random search baseline\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: Bayesian Optimization vs Random Search\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Bayesian Optimization results\n",
    "bo_best_ctr = trials_df['ctr'].max()\n",
    "bo_trials = len(trials_df)\n",
    "bo_avg_ctr = trials_df['ctr'].mean()\n",
    "\n",
    "# Simulate random search with same budget\n",
    "np.random.seed(42)\n",
    "random_ctrs = []\n",
    "for _ in range(bo_trials):\n",
    "    random_params = {\n",
    "        'learning_rate': np.random.uniform(1e-4, 1e-1),\n",
    "        'max_depth': np.random.randint(3, 11),\n",
    "        'min_samples_split': np.random.randint(2, 21),\n",
    "        'optimizer': np.random.choice(['adam', 'sgd', 'rmsprop'])\n",
    "    }\n",
    "    ctr = simulate_true_objective(random_params)\n",
    "    random_ctrs.append(ctr)\n",
    "\n",
    "random_best_ctr = max(random_ctrs)\n",
    "random_avg_ctr = np.mean(random_ctrs)\n",
    "\n",
    "print(f\"\\nBayesian Optimization:\")\n",
    "print(f\"  Trials: {bo_trials}\")\n",
    "print(f\"  Best CTR: {bo_best_ctr:.4f}\")\n",
    "print(f\"  Average CTR: {bo_avg_ctr:.4f}\")\n",
    "\n",
    "print(f\"\\nRandom Search (baseline):\")\n",
    "print(f\"  Trials: {bo_trials}\")\n",
    "print(f\"  Best CTR: {random_best_ctr:.4f}\")\n",
    "print(f\"  Average CTR: {random_avg_ctr:.4f}\")\n",
    "\n",
    "improvement = ((bo_best_ctr - random_best_ctr) / random_best_ctr) * 100\n",
    "avg_improvement = ((bo_avg_ctr - random_avg_ctr) / random_avg_ctr) * 100\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Best CTR improvement: {improvement:+.2f}%\")\n",
    "print(f\"  Average CTR improvement: {avg_improvement:+.2f}%\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(f\"\\n✓ Bayesian Optimization found a better solution than random search!\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Random search performed better (may need more trials or tuning)\")\n",
    "\n",
    "# Check if we found near-optimal solution\n",
    "optimal_ctr = simulate_true_objective({\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_split': 6,\n",
    "    'optimizer': 'adam'\n",
    "})\n",
    "\n",
    "optimality_gap = ((optimal_ctr - bo_best_ctr) / optimal_ctr) * 100\n",
    "\n",
    "print(f\"\\nOptimality Analysis:\")\n",
    "print(f\"  True optimal CTR (known): {optimal_ctr:.4f}\")\n",
    "print(f\"  Found CTR: {bo_best_ctr:.4f}\")\n",
    "print(f\"  Optimality gap: {optimality_gap:.2f}%\")\n",
    "\n",
    "if optimality_gap < 5:\n",
    "    print(f\"\\n✓ Found near-optimal solution (within 5% of true optimum)!\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Solution is {optimality_gap:.1f}% away from optimum (may need more trials)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
