{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3fe5f4",
   "metadata": {},
   "source": [
    "# Chapter 4: Metric Design and Variance Reduction\n",
    "\n",
    "This notebook demonstrated four key techniques for metric design and variance reduction:\n",
    "\n",
    "1. **Proxy Metric Validation**: Using correlation analysis to select the best leading indicator for long-term business goals\n",
    "2. **Variance Comparison**: Understanding why different metric types (binomial, count, continuous) have dramatically different variances\n",
    "3. **CUPED**: Implementing the most powerful variance reduction technique using pre-experiment data\n",
    "4. **Metric Transformation**: Applying winsorization and log transformation to handle skewed distributions with outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045ecabc",
   "metadata": {},
   "source": [
    "## Setup: Install Required Packages\n",
    "\n",
    "If running for the first time, uncomment the following line to install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6730cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if packages are not installed\n",
    "# !pip install pandas numpy scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa302e",
   "metadata": {},
   "source": [
    "## Section 1.2: Validating Proxy Metrics - Correlation Analysis\n",
    "\n",
    "This example demonstrates how to compute a correlation table to evaluate candidate proxy metrics against a lagging indicator (12-month LTV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101850e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample historical user data (first month metrics + 12-month outcomes)\n",
    "# In practice, this would come from your data warehouse\n",
    "np.random.seed(42)\n",
    "n_users = 10000\n",
    "\n",
    "user_data = pd.DataFrame({\n",
    "    'user_id': range(n_users),\n",
    "    # First month metrics (potential proxies) - simulated for demonstration\n",
    "    'weekly_revenue_per_user': np.random.exponential(20, n_users),\n",
    "    'session_success_rate': np.random.beta(2, 8, n_users),\n",
    "    'items_per_cart': np.random.poisson(2.5, n_users),\n",
    "    'number_of_wishlist_adds': np.random.poisson(1.2, n_users),\n",
    "    'number_of_searches_per_week': np.random.poisson(3, n_users),\n",
    "    'time_spent_on_product_pages': np.random.exponential(120, n_users),\n",
    "    # Lagging indicator (measured 12 months later)\n",
    "    # In reality, LTV is correlated with early behavior\n",
    "    'ltv_12_months': np.random.exponential(150, n_users)\n",
    "})\n",
    "\n",
    "# Make LTV correlated with early metrics (simulating real relationship)\n",
    "user_data['ltv_12_months'] += (\n",
    "    0.7 * user_data['weekly_revenue_per_user'] +\n",
    "    0.5 * user_data['session_success_rate'] * 100 +\n",
    "    0.3 * user_data['items_per_cart'] * 10\n",
    ")\n",
    "\n",
    "# Calculate correlation of each proxy with the lagging indicator\n",
    "proxy_metrics = [\n",
    "    'weekly_revenue_per_user',\n",
    "    'session_success_rate', \n",
    "    'items_per_cart',\n",
    "    'number_of_wishlist_adds',\n",
    "    'number_of_searches_per_week',\n",
    "    'time_spent_on_product_pages'\n",
    "]\n",
    "\n",
    "correlation_results = []\n",
    "for metric in proxy_metrics:\n",
    "    corr = user_data[metric].corr(user_data['ltv_12_months'])\n",
    "    correlation_results.append({'Proxy Metric': metric, 'Correlation': corr})\n",
    "\n",
    "# Display results sorted by correlation strength\n",
    "results_df = pd.DataFrame(correlation_results).sort_values('Correlation', ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270a956",
   "metadata": {},
   "source": [
    "## Section 3.1: The Variance Problem - Comparing Metric Types\n",
    "\n",
    "This example computes variance for three different metric types (binomial, count, continuous) and demonstrates why revenue metrics require much larger sample sizes than conversion metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulate 10,000 users with different metric types\n",
    "n_users = 10000\n",
    "\n",
    "# 1. Binomial Metric: Conversion (0 or 1)\n",
    "conversion_rate = 0.05\n",
    "conversions = np.random.binomial(1, conversion_rate, n_users)\n",
    "var_binomial = np.var(conversions, ddof=1)\n",
    "print(f\"Binomial (Conversion) - Variance: {var_binomial:.4f}\")\n",
    "print(f\"  Theoretical: p(1-p) = {conversion_rate * (1-conversion_rate):.4f}\\n\")\n",
    "\n",
    "# 2. Count Metric: Number of purchases per user\n",
    "# 95% make 0, 4% make 1, 1% make 2 purchases\n",
    "purchase_counts = np.random.choice([0, 1, 2], n_users, p=[0.95, 0.04, 0.01])\n",
    "var_count = np.var(purchase_counts, ddof=1)\n",
    "expected_count = 0.95*0 + 0.04*1 + 0.01*2\n",
    "expected_squared = 0.95*0**2 + 0.04*1**2 + 0.01*2**2\n",
    "print(f\"Count (Purchases) - Variance: {var_count:.4f}\")\n",
    "print(f\"  Theoretical: E[C²] - E[C]² = {expected_squared - expected_count**2:.4f}\\n\")\n",
    "\n",
    "# 3. Continuous Metric: Revenue per user\n",
    "# Most users spend $0, converters spend between $10-$100, rare \"whales\" spend $500+\n",
    "revenue = np.zeros(n_users)\n",
    "converters = np.random.rand(n_users) < conversion_rate\n",
    "revenue[converters] = np.random.exponential(50, converters.sum())\n",
    "# Add a few whale users\n",
    "whale_mask = np.random.rand(n_users) < 0.001\n",
    "revenue[whale_mask] = np.random.uniform(500, 1000, whale_mask.sum())\n",
    "var_revenue = np.var(revenue, ddof=1)\n",
    "print(f\"Continuous (Revenue) - Variance: {var_revenue:.2f}\")\n",
    "print(f\"  Standard deviation: ${np.sqrt(var_revenue):.2f}\")\n",
    "print(f\"  Mean revenue: ${np.mean(revenue):.2f}\\n\")\n",
    "\n",
    "# Comparison: Required sample size scales with variance\n",
    "print(\"Relative sample size requirements (holding MDE constant):\")\n",
    "print(f\"  Binomial: 1.0x (baseline)\")\n",
    "print(f\"  Count: {var_count/var_binomial:.1f}x\")\n",
    "print(f\"  Revenue: {var_revenue/var_binomial:.0f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3678824",
   "metadata": {},
   "source": [
    "## Section 4.3: CUPED Implementation\n",
    "\n",
    "This example demonstrates a complete implementation of CUPED (Controlled-experiment Using Pre-Experiment Data) for variance reduction in A/B testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a3b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data: experiment results with pre-experiment covariate\n",
    "data = pd.DataFrame({\n",
    "    'user_id': range(1000),\n",
    "    'group': np.random.choice(['control', 'treatment'], 1000),\n",
    "    'revenue_in_experiment': np.random.exponential(50, 1000),  # Y\n",
    "    'revenue_30_days_prior': np.random.exponential(50, 1000)   # X (covariate)\n",
    "})\n",
    "\n",
    "# Introduce a treatment effect: +5% revenue for treatment group\n",
    "treatment_mask = data['group'] == 'treatment'\n",
    "data.loc[treatment_mask, 'revenue_in_experiment'] *= 1.05\n",
    "\n",
    "# Step 1: Define the outcome metric (Y) and covariate (X)\n",
    "Y = data['revenue_in_experiment'].values\n",
    "X = data['revenue_30_days_prior'].values\n",
    "\n",
    "# Step 2: Calculate covariance and variance (pooled across both groups)\n",
    "cov_XY = np.cov(X, Y)[0, 1]  # Covariance between X and Y\n",
    "var_X = np.var(X, ddof=1)     # Variance of X\n",
    "\n",
    "# Step 3: Calculate optimal theta\n",
    "theta = cov_XY / var_X\n",
    "print(f\"Optimal theta: {theta:.4f}\")\n",
    "print(f\"Correlation: {np.corrcoef(X, Y)[0, 1]:.4f}\")\n",
    "\n",
    "# Step 4: Compute CUPED-adjusted metric for each user\n",
    "data['Y_cuped'] = data['revenue_in_experiment'] - theta * data['revenue_30_days_prior']\n",
    "\n",
    "# Step 5: Compare variance reduction\n",
    "var_original = data['revenue_in_experiment'].var()\n",
    "var_cuped = data['Y_cuped'].var()\n",
    "variance_reduction = (1 - var_cuped / var_original) * 100\n",
    "print(f\"\\nVariance Reduction: {variance_reduction:.1f}%\")\n",
    "\n",
    "# Step 6: Perform t-test on original and CUPED-adjusted metrics\n",
    "control_original = data[data['group'] == 'control']['revenue_in_experiment']\n",
    "treatment_original = data[data['group'] == 'treatment']['revenue_in_experiment']\n",
    "\n",
    "control_cuped = data[data['group'] == 'control']['Y_cuped']\n",
    "treatment_cuped = data[data['group'] == 'treatment']['Y_cuped']\n",
    "\n",
    "# Original t-test\n",
    "t_stat_orig, p_val_orig = stats.ttest_ind(treatment_original, control_original)\n",
    "print(f\"\\n--- Original Metric ---\")\n",
    "print(f\"Control Mean: ${control_original.mean():.2f}\")\n",
    "print(f\"Treatment Mean: ${treatment_original.mean():.2f}\")\n",
    "print(f\"p-value: {p_val_orig:.4f}\")\n",
    "\n",
    "# CUPED-adjusted t-test\n",
    "t_stat_cuped, p_val_cuped = stats.ttest_ind(treatment_cuped, control_cuped)\n",
    "print(f\"\\n--- CUPED-Adjusted Metric ---\")\n",
    "print(f\"Control Mean: ${control_cuped.mean():.2f}\")\n",
    "print(f\"Treatment Mean: ${treatment_cuped.mean():.2f}\")\n",
    "print(f\"p-value: {p_val_cuped:.4f}\")\n",
    "print(f\"\\nSensitivity improvement: {p_val_orig / p_val_cuped:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879bdfe",
   "metadata": {},
   "source": [
    "## Section 5.2: Metric Transformation - Winsorization and Log Transformation\n",
    "\n",
    "This example demonstrates how winsorization and log transformation can reduce variance in skewed metrics with outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee58c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Generate skewed revenue data with outliers\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "# Most users spend little, some spend moderate amounts, a few are whales\n",
    "revenue = np.concatenate([\n",
    "    np.random.exponential(20, 950),      # 95% normal users\n",
    "    np.random.exponential(100, 45),      # 4.5% higher spenders\n",
    "    np.random.uniform(500, 2000, 5)      # 0.5% whales\n",
    "])\n",
    "\n",
    "print(\"Original Revenue Metric:\")\n",
    "print(f\"  Mean: ${revenue.mean():.2f}\")\n",
    "print(f\"  Std Dev: ${revenue.std():.2f}\")\n",
    "print(f\"  Variance: {revenue.var():.2f}\\n\")\n",
    "\n",
    "# Apply Winsorization at 99th percentile\n",
    "percentile_99 = np.percentile(revenue, 99)\n",
    "revenue_winsorized = np.clip(revenue, 0, percentile_99)\n",
    "print(f\"Winsorized Revenue (capped at 99th percentile: ${percentile_99:.2f}):\")\n",
    "print(f\"  Mean: ${revenue_winsorized.mean():.2f}\")\n",
    "print(f\"  Std Dev: ${revenue_winsorized.std():.2f}\")\n",
    "print(f\"  Variance: {revenue_winsorized.var():.2f}\")\n",
    "print(f\"  Variance reduction: {(1 - revenue_winsorized.var()/revenue.var())*100:.1f}%\\n\")\n",
    "\n",
    "# Apply Log Transformation (log(revenue + 1) to handle zeros)\n",
    "revenue_log = np.log(revenue + 1)\n",
    "print(f\"Log-Transformed Revenue (log(revenue + 1)):\")\n",
    "print(f\"  Mean: {revenue_log.mean():.4f}\")\n",
    "print(f\"  Std Dev: {revenue_log.std():.4f}\")\n",
    "print(f\"  Variance: {revenue_log.var():.4f}\")\n",
    "print(f\"  Variance reduction: {(1 - revenue_log.var()/revenue.var())*100:.1f}%\\n\")\n",
    "\n",
    "# Demonstrate impact on hypothesis testing\n",
    "# Split into two groups (simulate A/B test with 5% revenue lift in B)\n",
    "group_a = revenue[:500]\n",
    "group_b = revenue[500:] * 1.05  # 5% lift\n",
    "\n",
    "# Test on original metric\n",
    "t_stat_orig, p_val_orig = stats.ttest_ind(group_b, group_a)\n",
    "print(f\"T-test on Original Revenue: p-value = {p_val_orig:.4f}\")\n",
    "\n",
    "# Test on winsorized metric\n",
    "group_a_wins = np.clip(group_a, 0, percentile_99)\n",
    "group_b_wins = np.clip(group_b, 0, percentile_99)\n",
    "t_stat_wins, p_val_wins = stats.ttest_ind(group_b_wins, group_a_wins)\n",
    "print(f\"T-test on Winsorized Revenue: p-value = {p_val_wins:.4f}\")\n",
    "\n",
    "# Test on log-transformed metric\n",
    "group_a_log = np.log(group_a + 1)\n",
    "group_b_log = np.log(group_b + 1)\n",
    "t_stat_log, p_val_log = stats.ttest_ind(group_b_log, group_a_log)\n",
    "print(f\"T-test on Log Revenue: p-value = {p_val_log:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
