{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddbe9a47",
   "metadata": {},
   "source": [
    "# Chapter 18: Machine Learning from Experiment - Counterfactual Learning\n",
    "\n",
    "This notebook contains updated code examples from Chapter 18, demonstrating counterfactual learning methods for training ML models from experiment data.\n",
    "\n",
    "**Key Updates:**\n",
    "- Section 2.3: Improved uncertainty quantification methods\n",
    "- Section 3: Practical applications with complete workflows\n",
    "- Risk-aware decision strategies in Use Case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d40f8",
   "metadata": {},
   "source": [
    "## Setup: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas numpy scikit-learn econml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3115d24",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f25999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab8aa2c",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2.3: Quantifying Uncertainty in Treatment Effect Predictions\n",
    "\n",
    "Two methods for uncertainty quantification applicable to counterfactual learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61012d52",
   "metadata": {},
   "source": [
    "### Method 1: Model's Built-in Uncertainty (for ensemble models)\n",
    "\n",
    "Works with Random Forests, ExtraTrees, BaggingRegressor, and other ensemble models with accessible individual estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0602f6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ensemble uncertainty function defined\n"
     ]
    }
   ],
   "source": [
    "def predict_with_uncertainty_ensemble(model, X):\n",
    "    \"\"\"\n",
    "    Use ensemble model's individual estimators to estimate prediction uncertainty.\n",
    "    Example shown for Random Forest; similar approach works for other ensemble methods.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ensemble model with .estimators_ attribute\n",
    "        X: Features for prediction\n",
    "        \n",
    "    Returns:\n",
    "        mean: Point prediction (average across estimators)\n",
    "        std: Standard error (variance across estimators)\n",
    "        ci_lower, ci_upper: 95% confidence interval\n",
    "    \"\"\"\n",
    "    # Get predictions from each estimator (e.g., each tree in Random Forest)\n",
    "    estimator_predictions = np.array([est.predict(X) for est in model.estimators_])\n",
    "    \n",
    "    mean_pred = estimator_predictions.mean(axis=0)[0]\n",
    "    std_pred = estimator_predictions.std(axis=0)[0]\n",
    "    \n",
    "    # 95% confidence interval\n",
    "    ci_lower = np.percentile(estimator_predictions, 2.5, axis=0)[0]\n",
    "    ci_upper = np.percentile(estimator_predictions, 97.5, axis=0)[0]\n",
    "    \n",
    "    return mean_pred, std_pred, ci_lower, ci_upper\n",
    "\n",
    "print(\"✓ Ensemble uncertainty function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc642b",
   "metadata": {},
   "source": [
    "### Method 2: Bootstrap Resampling (Universal, more robust for small data)\n",
    "\n",
    "Works with ANY model type and provides more accurate uncertainty estimates with small experiment datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73a08bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Bootstrap uncertainty function defined\n"
     ]
    }
   ],
   "source": [
    "def predict_with_bootstrap(model_class, model_params, X_train, y_train, X_test, n_bootstrap=100):\n",
    "    \"\"\"\n",
    "    Bootstrap training data to estimate prediction uncertainty.\n",
    "    Works with ANY model type: Random Forests, XGBoost, Linear Models, Neural Networks, etc.\n",
    "    \n",
    "    Args:\n",
    "        model_class: Model class (e.g., RandomForestRegressor, XGBRegressor, LinearRegression)\n",
    "        model_params: Dict of hyperparameters for model instantiation\n",
    "        X_train, y_train: Training data\n",
    "        X_test: Test data for prediction\n",
    "        n_bootstrap: Number of bootstrap resamples (typically 50-200)\n",
    "        \n",
    "    Returns:\n",
    "        mean, std, ci_lower, ci_upper\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        # Resample training data with replacement\n",
    "        X_boot, y_boot = resample(X_train, y_train, random_state=i)\n",
    "        \n",
    "        # Train model on bootstrap sample\n",
    "        model = model_class(**model_params, random_state=i)\n",
    "        model.fit(X_boot, y_boot)\n",
    "        \n",
    "        # Predict on test data\n",
    "        pred = model.predict(X_test)[0]\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    mean_pred = predictions.mean()\n",
    "    std_pred = predictions.std()\n",
    "    ci_lower = np.percentile(predictions, 2.5)\n",
    "    ci_upper = np.percentile(predictions, 97.5)\n",
    "    \n",
    "    return mean_pred, std_pred, ci_lower, ci_upper\n",
    "\n",
    "print(\"✓ Bootstrap uncertainty function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d157fd56",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3.1: Use Case 1 - Personalized Recommendations\n",
    "\n",
    "**Goal**: Predict which users benefit most from personalized recommendations vs popular items.\n",
    "\n",
    "**Complete 4-Step Workflow:**\n",
    "1. Collect experiment data\n",
    "2. Train T-Learner (separate models for treatment/control)\n",
    "3. Predict individual treatment effects (point estimates)\n",
    "4. Quantify uncertainty and apply risk-aware decision strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a996fe",
   "metadata": {},
   "source": [
    "### Step 1: Collect Experiment Data\n",
    "\n",
    "Generate synthetic experiment data with user features, treatment assignment, and CTR outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "708fd1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5000 experiment observations\n",
      "\n",
      "Treatment distribution:\n",
      "treatment\n",
      "0    2501\n",
      "1    2499\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data:\n",
      "   user_id  age  num_past_purchases  treatment       ctr\n",
      "0        0   56                   8          0  0.050006\n",
      "1        1   46                  10          1  0.048332\n",
      "2        2   32                   6          1  0.050216\n",
      "3        3   60                   6          1  0.055505\n",
      "4        4   25                   5          1  0.066964\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic experiment data\n",
    "np.random.seed(42)\n",
    "n_users = 5000\n",
    "\n",
    "# User features\n",
    "ages = np.random.randint(18, 65, n_users)\n",
    "past_purchases = np.random.poisson(5, n_users)\n",
    "\n",
    "# Treatment assignment (50/50 split)\n",
    "treatment = np.random.binomial(1, 0.5, n_users)\n",
    "\n",
    "# True heterogeneous treatment effect (varies by user)\n",
    "# Young users with few purchases benefit more from personalization\n",
    "base_ctr = 0.05\n",
    "personalization_effect = 0.02 * (40 - ages) / 40 + 0.01 * (10 - past_purchases) / 10\n",
    "personalization_effect = np.maximum(personalization_effect, 0)  # Non-negative\n",
    "\n",
    "# Observed CTR\n",
    "ctr = np.where(treatment == 1, \n",
    "               base_ctr + personalization_effect + np.random.normal(0, 0.005, n_users),\n",
    "               base_ctr + np.random.normal(0, 0.005, n_users))\n",
    "ctr = np.clip(ctr, 0, 1)  # Keep in [0, 1]\n",
    "\n",
    "# Create DataFrame\n",
    "experiment_data = pd.DataFrame({\n",
    "    'user_id': range(n_users),\n",
    "    'age': ages,\n",
    "    'num_past_purchases': past_purchases,\n",
    "    'treatment': treatment,  # 1=personalized, 0=popular\n",
    "    'ctr': ctr\n",
    "})\n",
    "\n",
    "print(f\"Generated {len(experiment_data)} experiment observations\")\n",
    "print(f\"\\nTreatment distribution:\")\n",
    "print(experiment_data['treatment'].value_counts())\n",
    "print(f\"\\nSample data:\")\n",
    "print(experiment_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716263a4",
   "metadata": {},
   "source": [
    "### Step 2: Train a Counterfactual Model (T-Learner)\n",
    "\n",
    "Train separate Random Forest models on treatment and control data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1d628f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ T-Learner models trained\n",
      "  Treatment model: 2499 samples\n",
      "  Control model: 2501 samples\n"
     ]
    }
   ],
   "source": [
    "# Separate treatment and control data\n",
    "treatment_data = experiment_data[experiment_data['treatment'] == 1]\n",
    "control_data = experiment_data[experiment_data['treatment'] == 0]\n",
    "\n",
    "X_features = ['age', 'num_past_purchases']\n",
    "\n",
    "# Train two models\n",
    "model_treatment = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_treatment.fit(treatment_data[X_features], treatment_data['ctr'])\n",
    "\n",
    "model_control = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_control.fit(control_data[X_features], control_data['ctr'])\n",
    "\n",
    "print(\"✓ T-Learner models trained\")\n",
    "print(f\"  Treatment model: {len(treatment_data)} samples\")\n",
    "print(f\"  Control model: {len(control_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea5f1d1",
   "metadata": {},
   "source": [
    "### Step 3: Predict Individual Treatment Effects (Point Estimates)\n",
    "\n",
    "Make predictions for a new user and apply a simple threshold-based decision rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32eaf88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted CTR (personalized): 0.0610\n",
      "Predicted CTR (popular): 0.0516\n",
      "Expected uplift: 0.0094\n",
      "\n",
      "✗ Recommend: Popular (uplift 0.0094 ≤ threshold 0.01)\n"
     ]
    }
   ],
   "source": [
    "# For a new user, predict CTR under both scenarios\n",
    "new_user = pd.DataFrame({'age': [30], 'num_past_purchases': [7]})\n",
    "\n",
    "ctr_personalized = model_treatment.predict(new_user)[0]\n",
    "ctr_popular = model_control.predict(new_user)[0]\n",
    "\n",
    "uplift = ctr_personalized - ctr_popular\n",
    "print(f\"Predicted CTR (personalized): {ctr_personalized:.4f}\")\n",
    "print(f\"Predicted CTR (popular): {ctr_popular:.4f}\")\n",
    "print(f\"Expected uplift: {uplift:.4f}\")\n",
    "\n",
    "# Simple decision rule: Show personalized recommendations if uplift > threshold\n",
    "threshold = 0.01\n",
    "if uplift > threshold:\n",
    "    print(f\"\\n✓ Recommend: Personalized (uplift {uplift:.4f} > threshold {threshold})\")\n",
    "else:\n",
    "    print(f\"\\n✗ Recommend: Popular (uplift {uplift:.4f} ≤ threshold {threshold})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c97fe",
   "metadata": {},
   "source": [
    "### Step 4: Quantifying Uncertainty and Risk-Aware Decisions\n",
    "\n",
    "Apply uncertainty quantification to get confidence intervals, then use risk-aware decision strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0251a58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected uplift: 0.0094 ± 0.0024\n",
      "95% CI: [0.0018, 0.0162]\n"
     ]
    }
   ],
   "source": [
    "# Using ensemble model's built-in uncertainty (Section 2.3)\n",
    "# Works because model_treatment and model_control are RandomForestRegressors\n",
    "ctr_pers_mean, ctr_pers_std, ctr_pers_lower, ctr_pers_upper = \\\n",
    "    predict_with_uncertainty_ensemble(model_treatment, new_user)\n",
    "\n",
    "ctr_pop_mean, ctr_pop_std, ctr_pop_lower, ctr_pop_upper = \\\n",
    "    predict_with_uncertainty_ensemble(model_control, new_user)\n",
    "\n",
    "# Compute uplift with uncertainty\n",
    "uplift_mean = ctr_pers_mean - ctr_pop_mean\n",
    "uplift_std = np.sqrt(ctr_pers_std**2 + ctr_pop_std**2)\n",
    "uplift_lower = ctr_pers_lower - ctr_pop_upper\n",
    "uplift_upper = ctr_pers_upper - ctr_pop_lower\n",
    "\n",
    "print(f\"Expected uplift: {uplift_mean:.4f} ± {uplift_std:.4f}\")\n",
    "print(f\"95% CI: [{uplift_lower:.4f}, {uplift_upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16342c",
   "metadata": {},
   "source": [
    "#### Risk-Aware Decision Strategies\n",
    "\n",
    "Now use the uncertainty estimates to make smarter decisions with 4 different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15d4b868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Decision Strategies ===\n",
      "Optimistic (point estimate):     Popular\n",
      "Conservative (high confidence):  Popular\n",
      "Thompson Sampling:               Popular\n",
      "UCB (exploration bonus):         Personalized\n",
      "Prediction certainty:            74.1%\n"
     ]
    }
   ],
   "source": [
    "def risk_aware_decision(uplift_mean, uplift_std, uplift_lower, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Make risk-aware treatment decisions using uncertainty estimates.\n",
    "    \n",
    "    Args:\n",
    "        uplift_mean: Point estimate of treatment effect\n",
    "        uplift_std: Standard error of effect estimate\n",
    "        uplift_lower: Lower bound of 95% confidence interval\n",
    "        threshold: Minimum effect size to justify treatment\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with decisions from 4 strategies\n",
    "    \"\"\"\n",
    "    # Strategy 1: Optimistic (use point estimate)\n",
    "    optimistic = \"Personalized\" if uplift_mean > threshold else \"Popular\"\n",
    "    \n",
    "    # Strategy 2: Conservative (require high confidence)\n",
    "    conservative = \"Personalized\" if uplift_lower > threshold else \"Popular\"\n",
    "    \n",
    "    # Strategy 3: Thompson Sampling (sample from distribution)\n",
    "    uplift_sample = np.random.normal(uplift_mean, uplift_std)\n",
    "    thompson = \"Personalized\" if uplift_sample > threshold else \"Popular\"\n",
    "    \n",
    "    # Strategy 4: Upper Confidence Bound (optimistic + exploration bonus)\n",
    "    ucb_value = uplift_mean + 1.96 * uplift_std\n",
    "    ucb = \"Personalized\" if ucb_value > threshold else \"Popular\"\n",
    "    \n",
    "    return {\n",
    "        'optimistic': optimistic,\n",
    "        'conservative': conservative,\n",
    "        'thompson_sampling': thompson,\n",
    "        'ucb': ucb,\n",
    "        'certainty': 1 - (uplift_std / abs(uplift_mean)) if uplift_mean != 0 else 0\n",
    "    }\n",
    "\n",
    "# Apply decision strategies\n",
    "decisions = risk_aware_decision(uplift_mean, uplift_std, uplift_lower)\n",
    "\n",
    "print(f\"\\n=== Decision Strategies ===\")\n",
    "print(f\"Optimistic (point estimate):     {decisions['optimistic']}\")\n",
    "print(f\"Conservative (high confidence):  {decisions['conservative']}\")\n",
    "print(f\"Thompson Sampling:               {decisions['thompson_sampling']}\")\n",
    "print(f\"UCB (exploration bonus):         {decisions['ucb']}\")\n",
    "print(f\"Prediction certainty:            {decisions['certainty']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d9c82e",
   "metadata": {},
   "source": [
    "#### When to Use Each Strategy\n",
    "\n",
    "1. **Optimistic (Point Estimate)**: Large sample sizes, low uncertainty, risk-tolerant applications\n",
    "2. **Conservative (Lower Bound)**: High-stakes decisions (medical, financial), requires strong evidence before acting\n",
    "3. **Thompson Sampling**: Ongoing learning systems, naturally balances exploration/exploitation when uncertain\n",
    "4. **Upper Confidence Bound (UCB)**: Similar to Thompson but deterministic, good for reproducible decisions\n",
    "\n",
    "**Practical Recommendation**: Start with Conservative strategy until you accumulate enough data to be confident, then switch to Optimistic for production efficiency. For continuously learning systems (recommenders, bandits), use Thompson Sampling to maintain exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0826415f",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3.2: Use Case 2 - Multi-Treatment Personalization\n",
    "\n",
    "Choose among 3+ options (e.g., discount vs free shipping vs bundle deal) by predicting which treatment yields highest uplift per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7b615d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated multi-treatment experiment data\n",
      "Treatment distribution:\n",
      "treatment\n",
      "bundle       509\n",
      "free_ship    502\n",
      "discount     489\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic multi-treatment experiment data\n",
    "np.random.seed(43)\n",
    "n_samples = 1500\n",
    "\n",
    "ages_multi = np.random.randint(18, 65, n_samples)\n",
    "past_purchases_multi = np.random.poisson(5, n_samples)\n",
    "\n",
    "# Three treatments: discount, free_ship, bundle\n",
    "treatments = np.random.choice(['discount', 'free_ship', 'bundle'], n_samples)\n",
    "\n",
    "# Treatment effects vary by user\n",
    "# Young users prefer discounts, frequent buyers prefer free shipping, occasional buyers like bundles\n",
    "base_conversion = 0.10\n",
    "treatment_effects = {\n",
    "    'discount': 0.08 * (40 - ages_multi) / 40,\n",
    "    'free_ship': 0.12 * (past_purchases_multi / 10),\n",
    "    'bundle': 0.06 * (1 - past_purchases_multi / 20)\n",
    "}\n",
    "\n",
    "conversion = base_conversion + np.array([treatment_effects[t][i] for i, t in enumerate(treatments)])\n",
    "conversion += np.random.normal(0, 0.02, n_samples)\n",
    "conversion = np.clip(conversion, 0, 1)\n",
    "\n",
    "multi_data = pd.DataFrame({\n",
    "    'age': ages_multi,\n",
    "    'past_purchases': past_purchases_multi,\n",
    "    'treatment': treatments,\n",
    "    'conversion': conversion\n",
    "})\n",
    "\n",
    "print(\"Generated multi-treatment experiment data\")\n",
    "print(f\"Treatment distribution:\\n{multi_data['treatment'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c561c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Multi-treatment models trained\n"
     ]
    }
   ],
   "source": [
    "# Train separate models for each treatment using T-Learner approach\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "treatments_list = ['discount', 'free_ship', 'bundle']\n",
    "treatment_models = {}\n",
    "\n",
    "X_features = ['age', 'past_purchases']\n",
    "\n",
    "for treatment in treatments_list:\n",
    "    treatment_subset = multi_data[multi_data['treatment'] == treatment]\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(treatment_subset[X_features], treatment_subset['conversion'])\n",
    "    treatment_models[treatment] = model\n",
    "\n",
    "print(\"✓ Multi-treatment models trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d419b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: age=25, past_purchases=3\n",
      "Predicted conversions: {'discount': np.float64(0.15648898849799242), 'free_ship': np.float64(0.14540964939371706), 'bundle': np.float64(0.1530752185676788)}\n",
      "✓ Recommended treatment: discount\n"
     ]
    }
   ],
   "source": [
    "# Predict uplift for each treatment, choose best\n",
    "def personalized_promotion(user_features, models):\n",
    "    \"\"\"Assign promotion based on predicted conversion rate.\"\"\"\n",
    "    predictions = {\n",
    "        treatment: model.predict(user_features)[0]\n",
    "        for treatment, model in models.items()\n",
    "    }\n",
    "    \n",
    "    best_treatment = max(predictions, key=predictions.get)\n",
    "    return best_treatment, predictions\n",
    "\n",
    "# Example user\n",
    "test_user = pd.DataFrame({'age': [25], 'past_purchases': [3]})\n",
    "recommended_treatment, all_predictions = personalized_promotion(test_user, treatment_models)\n",
    "\n",
    "print(f\"User: age=25, past_purchases=3\")\n",
    "print(f\"Predicted conversions: {all_predictions}\")\n",
    "print(f\"✓ Recommended treatment: {recommended_treatment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c28bc",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3.3: Use Case 3 - Model Calibration\n",
    "\n",
    "Correct systematic biases in existing predictive models using experiment data as ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59fd8d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated calibration experiment data\n",
      "Mean predicted CTR: 0.0997\n",
      "Mean actual CTR: 0.0801\n",
      "Bias: 0.0196\n"
     ]
    }
   ],
   "source": [
    "# Simulate biased model predictions and experiment ground truth\n",
    "np.random.seed(44)\n",
    "n_exp = 200\n",
    "\n",
    "# Experiment data: Model predictions vs actual outcomes\n",
    "predicted_ctr = np.random.uniform(0.05, 0.15, n_exp)\n",
    "# Model systematically overestimates for new users (simulate bias)\n",
    "actual_ctr = predicted_ctr * 0.8 + np.random.normal(0, 0.01, n_exp)\n",
    "actual_ctr = np.clip(actual_ctr, 0, 1)\n",
    "is_new_user = np.random.binomial(1, 0.3, n_exp)\n",
    "\n",
    "experiment_results = pd.DataFrame({\n",
    "    'predicted_ctr': predicted_ctr,\n",
    "    'actual_ctr': actual_ctr,\n",
    "    'is_new_user': is_new_user\n",
    "})\n",
    "\n",
    "print(\"Generated calibration experiment data\")\n",
    "print(f\"Mean predicted CTR: {predicted_ctr.mean():.4f}\")\n",
    "print(f\"Mean actual CTR: {actual_ctr.mean():.4f}\")\n",
    "print(f\"Bias: {(predicted_ctr.mean() - actual_ctr.mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a00a2507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calibration Example:\n",
      "Original prediction (new user): 0.1000\n",
      "Calibrated prediction: 0.0835\n",
      "Adjustment: -0.0165\n"
     ]
    }
   ],
   "source": [
    "# Train calibrator for new users\n",
    "new_user_data = experiment_results[experiment_results['is_new_user'] == 1]\n",
    "\n",
    "calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "calibrator.fit(new_user_data['predicted_ctr'], new_user_data['actual_ctr'])\n",
    "\n",
    "# Apply calibration\n",
    "def calibrated_ctr_prediction(predicted_ctr, is_new_user):\n",
    "    \"\"\"Wrap the original model with calibration.\"\"\"\n",
    "    if is_new_user:\n",
    "        return calibrator.predict([predicted_ctr])[0]\n",
    "    else:\n",
    "        return predicted_ctr\n",
    "\n",
    "# Test calibration\n",
    "test_prediction = 0.10\n",
    "calibrated = calibrated_ctr_prediction(test_prediction, is_new_user=True)\n",
    "\n",
    "print(f\"\\nCalibration Example:\")\n",
    "print(f\"Original prediction (new user): {test_prediction:.4f}\")\n",
    "print(f\"Calibrated prediction: {calibrated:.4f}\")\n",
    "print(f\"Adjustment: {(calibrated - test_prediction):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae9381",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3.4: Use Case 4 - Initializing Contextual Bandits\n",
    "\n",
    "Use experiment data to \"warm start\" a bandit policy, combining A/B test safety with adaptive learning efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9703f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Bandit Warm-Start:\n",
      "✓ Initial models trained on experiment data:\n",
      "  - Treatment model: 480 observations\n",
      "  - Control model: 520 observations\n",
      "\n",
      "✓ Deploy with Thompson Sampling:\n",
      "  - Use risk_aware_decision(..., strategy='thompson_sampling')\n",
      "  - Continues learning from online data\n",
      "  - Maintains exploration while exploiting learned patterns\n",
      "\n",
      "Benefits:\n",
      "  - Safe initialization from unbiased experiment data\n",
      "  - Adaptive learning in production\n",
      "  - Balances exploration/exploitation naturally\n"
     ]
    }
   ],
   "source": [
    "# Use experiment data from Use Case 1 to initialize a simple contextual model\n",
    "# This model can then be deployed with Thompson Sampling for ongoing adaptation\n",
    "\n",
    "print(\"Contextual Bandit Warm-Start:\")\n",
    "print(f\"✓ Initial models trained on experiment data:\")\n",
    "print(f\"  - Treatment model: {len(treatment_data)} observations\")\n",
    "print(f\"  - Control model: {len(control_data)} observations\")\n",
    "print(f\"\\n✓ Deploy with Thompson Sampling:\")\n",
    "print(f\"  - Use risk_aware_decision(..., strategy='thompson_sampling')\")\n",
    "print(f\"  - Continues learning from online data\")\n",
    "print(f\"  - Maintains exploration while exploiting learned patterns\")\n",
    "print(f\"\\nBenefits:\")\n",
    "print(f\"  - Safe initialization from unbiased experiment data\")\n",
    "print(f\"  - Adaptive learning in production\")\n",
    "print(f\"  - Balances exploration/exploitation naturally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc7480",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Section 2.3**: Two uncertainty quantification methods\n",
    "   - Ensemble built-in uncertainty (fast, for RF/ensemble models)\n",
    "   - Bootstrap resampling (universal, better for small data)\n",
    "\n",
    "2. **Section 3.1**: Complete personalized recommendations workflow\n",
    "   - 4-step process from data collection to risk-aware decisions\n",
    "   - 4 decision strategies with different risk profiles\n",
    "\n",
    "3. **Section 3.2**: Multi-treatment personalization\n",
    "   - Choosing best option among 3+ treatments per user\n",
    "\n",
    "4. **Section 3.3**: Model calibration\n",
    "   - Correcting systematic biases using experiment ground truth\n",
    "\n",
    "5. **Section 3.4**: Contextual bandits warm-start\n",
    "   - Combining A/B test safety with adaptive learning\n",
    "\n",
    "**Key Takeaway**: Experiment data provides clean causal signals for training ML models. Quantifying uncertainty and using risk-aware decision strategies ensures robust deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
